{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f0bec9de",
      "metadata": {},
      "source": [
        "# Chat sanity check (Unsloth)\n",
        "\n",
        "This notebook loads the LoRA adapter + tokenizer from `outputs_chess_distill` and lets you\n",
        "quickly test whether chat capabilities were preserved after vocabulary changes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e43bf576",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_8893/2410452886.py:4: UserWarning: WARNING: Unsloth should be imported before [transformers, peft] to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
            "\n",
            "Please restructure your imports with 'import unsloth' at the top of your file.\n",
            "  from unsloth import FastLanguageModel\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2026.1.2: Fast Qwen2 patching. Transformers: 4.56.2.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:1225: UserWarning: Model has `tie_word_embeddings=True` and a tied layer is part of the adapter, but `ensure_weight_tying` is not set to True. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. Check the discussion here: https://github.com/huggingface/peft/issues/2777\n",
            "  warnings.warn(msg)\n",
            "Unsloth 2026.1.2 patched 24 layers with 24 QKV layers, 24 O layers and 24 MLP layers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer vocab size: 153635\n",
            "Model embedding size: 153635\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "MODEL_DIR = Path(\"outputs_chess_distill\")\n",
        "MAX_SEQ_LENGTH = 2048\n",
        "LOAD_IN_4BIT = torch.cuda.is_available()\n",
        "DTYPE = None  # Let Unsloth pick a sensible dtype\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=str(MODEL_DIR),\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dtype=DTYPE,\n",
        "    load_in_4bit=LOAD_IN_4BIT,\n",
        ")\n",
        "\n",
        "# Ensure embeddings match the extended tokenizer vocabulary.\n",
        "input_emb = model.get_input_embeddings()\n",
        "if input_emb is not None and input_emb.weight.shape[0] != len(tokenizer):\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "    if hasattr(model, \"tie_weights\"):\n",
        "        model.tie_weights()\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "model.eval()\n",
        "\n",
        "print(\"Tokenizer vocab size:\", len(tokenizer))\n",
        "print(\"Model embedding size:\", model.get_input_embeddings().weight.shape[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2ae3c5b5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def _format_chat(messages):\n",
        "    if hasattr(tokenizer, \"apply_chat_template\"):\n",
        "        return tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True,\n",
        "        )\n",
        "    # Fallback for tokenizers without a chat template.\n",
        "    prompt_lines = [f\"{m['role']}: {m['content']}\" for m in messages]\n",
        "    return \"\\n\".join(prompt_lines) + \"\\nassistant:\"\n",
        "\n",
        "@torch.inference_mode()\n",
        "def chat(messages, max_new_tokens=256, temperature=0.7, top_p=0.9):\n",
        "    prompt = _format_chat(messages)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=temperature > 0,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    gen_ids = outputs[0][inputs[\"input_ids\"].shape[1]:]\n",
        "    return tokenizer.decode(gen_ids, skip_special_tokens=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3c306512",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "When you get up to your knees on the floor,g7h8biving it all to the winds, you can't moveg7h8bore thanc4f4g5f6h4g3g6g5g5g4h6h5h7h6g7g5f4f3f7f6g7g6g6f6f6f5h7h5g7g6h7g8h6h5g6g7h4h3g7g6g5g4g4g3h7h5g6f7g6g5h7h66g6g5g4g3g6h5g5g4h6h5g6g5g7g6g6g5f5f4g6g5f6g6g6g5g5g4h6h5g6g5g5g4f7f6h6g6g7g6h6g5h6h5h5h4f6f7g6h7h5g4g6g5f4f3g5g4g6f6g7g6f7f6h5h4g5g6h6h5f4g5g5g4g6f5h5h4f4f3g6f6g7g6h5h4g6f6g5g4g6g5f4f3g6g5f4f3g6g5h5h6f4g4g6g5f5f4h6h7g7g6f7f6g6g5g5g4g7g6hÂÖ≠ÂÖ≠ÂÖ≠g6g5g6g5h5h4g7g6g6h6ÂÖ≠ÂçÅÂÖ≠‰∏™ÂÖ≠ÂÖ≠ÂÖ≠ÂçÅg6g5ÂÖ≠‰∏™\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a concise, friendly assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Say hello and tell me one fun fact about chess.\"},\n",
        "]\n",
        "response = chat(messages, max_new_tokens=128)\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c55dc14a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional quick chat loop (type 'exit' to stop).\n",
        "def chat_loop(system_prompt=\"You are a helpful assistant.\"):\n",
        "    messages = []\n",
        "    if system_prompt:\n",
        "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
        "    while True:\n",
        "        user_text = input(\"User: \")\n",
        "        if not user_text or user_text.strip().lower() in {\"exit\", \"quit\"}:\n",
        "            break\n",
        "        messages.append({\"role\": \"user\", \"content\": user_text})\n",
        "        reply = chat(messages)\n",
        "        print(\"Assistant:\", reply)\n",
        "        messages.append({\"role\": \"assistant\", \"content\": reply})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c595a099",
      "metadata": {},
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a concise, friendly assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Say hello and tell me one fun fact about chess.\"},\n",
        "]\n",
        "response = chat(messages, max_new_tokens=128)\n",
        "print(response)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cloudspace",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
